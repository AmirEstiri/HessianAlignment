{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9ce0ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Copyright (C) 2019. Huawei Technologies Co., Ltd. All rights reserved.\n",
    "This program is free software; you can redistribute it and/or modify\n",
    "it under the terms of the Apache 2.0 License.\n",
    "This program is distributed in the hope that it will be useful,\n",
    "but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n",
    "Apache 2.0 License for more details.\n",
    "'''\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import torch\n",
    "from utils.aggregate import aggregate, aggregate_lr, zero_model, aggregate_momentum\n",
    "from algs.individual_train import individual_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "621b6ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FedAvg(torch.nn.Module):  # abstract class for FL algorithms\n",
    "    \"\"\"subclass should implement the following\n",
    "    --aggregate(): the server aggregation of models\n",
    "    --local_updates(): the update of each client\"\"\"\n",
    "    def __init__(self, models, optimizers, num_clients, num_local_epochs, loss_func):\n",
    "        super(FedAvg, self).__init__()\n",
    "        self.num_clients = num_clients\n",
    "        self.num_local_epochs = num_local_epochs\n",
    "        self.models = models\n",
    "        self.optimizers = optimizers\n",
    "        self.loss_func = loss_func\n",
    "        self.losses = None\n",
    "    \n",
    "    def aggregate(self, weights=None):\n",
    "        aggregate(models, weights=weights)\n",
    "    \n",
    "    def local_updates(self, train_loaders, test_loaders, device, output_dir):\n",
    "        loss_func = self.loss_func\n",
    "        self.losses = losses(self.models, train_loaders, self.loss_func, device)\n",
    "        for i in range(self.num_clients):\n",
    "            individual_train(train_loaders[i], loss_func, self.optimizers[i], self.models[i], \\\n",
    "                             test_loaders[i], \\\n",
    "                         device=device, client_id=i, epochs=self.num_local_epochs, \\\n",
    "                         output_dir=output_dir, show=False, save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e5f5208",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.project import project\n",
    "class AFL(FedAvg):\n",
    "    def __init__(self, models, optimizers, num_clients, num_local_epochs, \\\n",
    "                 loss_func, lambda_, step_size_lambda=0.1):\n",
    "        super(AFL, self).__init__(models, optimizers, num_clients, num_local_epochs, loss_func)\n",
    "        self.lambda_ = lambda_\n",
    "        self.step_size_lambda = step_size_lambda\n",
    "    \n",
    "    def aggregate(self, weights):\n",
    "        # update lamdba\n",
    "        self.lambda_ = project(np.array(self.lambda_) + self.step_size_lambda * \\\n",
    "                               np.array(self.losses))\n",
    "        super(AFL, self).aggregate(weights=self.lambda_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dd8a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "PropFair(models, optimizers, args.num_clients, args.num_local_epochs, \\\n",
    "             loss_func, base=5.0, epsilon = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "762e08c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PropFair(FedAvg):\n",
    "    def __init__(self, models, optimizers, num_clients, num_local_epochs, \\\n",
    "             loss_func, base, epsilon = 0.2):\n",
    "        super(PropFair, self).__init__(models, optimizers, \\\n",
    "                                       num_clients, num_local_epochs, loss_func)\n",
    "        self.base = base\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def local_updates(self, train_loaders, test_loaders, device, output_dir):\n",
    "        \n",
    "        def log_loss(output, target, base=self.base):\n",
    "            ce_loss = self.loss_func(output, target)\n",
    "            base = torch.tensor(base).to(device)\n",
    "            if base - ce_loss < self.epsilon:           \n",
    "                # for the bad performing batches, we enforce a constant to avoid divergence\n",
    "                return ce_loss/base\n",
    "            else:\n",
    "                return -torch.log(1 - ce_loss/base)\n",
    "        \n",
    "        loss_func = log_loss\n",
    "        for i in range(self.num_clients):\n",
    "            individual_train(train_loaders[i], loss_func, self.optimizers[i], self.models[i], \\\n",
    "                             test_loaders[i], \\\n",
    "                         device=device, client_id=i, epochs=self.num_local_epochs, \\\n",
    "                         output_dir=output_dir, show=False, save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ba77c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class qFFL(FedAvg):\n",
    "    def __init__(self, models, optimizers, num_clients, num_local_epochs, \\\n",
    "             loss_func, q=1.0):\n",
    "        super(qFFL, self).__init__(models, optimizers, num_clients, num_local_epochs, loss_func)\n",
    "        self.q = q\n",
    "        \n",
    "    def local_updates(self, train_loaders, test_loaders, device, output_dir):\n",
    "        \n",
    "        def q_loss(output, target, q=self.q):\n",
    "            ce_loss = loss_func(output, target)\n",
    "            return ce_loss ** (q + 1.0) / (q + 1.0)\n",
    "        \n",
    "        loss_func = q_loss\n",
    "        for i in range(self.num_clients):\n",
    "            individual_train(train_loaders[i], loss_func, self.optimizers[i], self.models[i], \\\n",
    "                             test_loaders[i], \\\n",
    "                         device=device, client_id=i, epochs=self.num_local_epochs, \\\n",
    "                         output_dir=output_dir, show=False, save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fc0c376",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from utils.aggregate import aggregate, sum_models, zero_model, \\\n",
    "                        assign_models, scale_model, sub_models, norm2_model\n",
    "\n",
    "class qFedAvg(FedAvg):\n",
    "    def __init__(self, models, optimizers, num_clients, num_local_epochs, \\\n",
    "             loss_func, Lipschitz, q=1.0):\n",
    "        super(qFedAvg, self).__init__(models, optimizers, num_clients, \\\n",
    "                                      num_local_epochs, loss_func)\n",
    "        self.q = q\n",
    "        self.Lipschitz = Lipschitz\n",
    "        self.old_models = deepcopy(models[0])\n",
    "        self.losses = None\n",
    "        \n",
    "    def local_updates(self, train_loaders, test_loaders, device, output_dir):\n",
    "        self.old_model = deepcopy(models[0])\n",
    "        super(qFedAvg, self).local_updates(train_loaders, test_loaders, device, output_dir)\n",
    "    \n",
    "    def aggregate(self, weights):\n",
    "        delta_w = [scale_model(sub_models(self.old_model, model), self.Lipschitz)\\\n",
    "                   for model in self.models]\n",
    "        Delta = [scale_model(delta_w[i], (self.losses[i] ** self.q)) for i in range(len(delta_w))]\n",
    "        h = [self.q * (self.losses[i] ** (self.q - 1)) * norm2_model(delta_w[i]) + \\\n",
    "               self.Lipschitz * (self.losses[i] ** self.q) for i in range(len(delta_w))]\n",
    "        new_model = sub_models(model, scale_model(sum_models(Delta), 1.0 / sum(h)))\n",
    "        assign_models(self.models, new_model)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f986c97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TERM(FedAvg):\n",
    "    def __init__(self, models, optimizers, num_clients, num_local_epochs, \\\n",
    "             loss_func, Lipschitz, alpha=0.1):\n",
    "        super(qFedAvg, self).__init__(models, optimizers, num_clients, \\\n",
    "                                      num_local_epochs, loss_func)\n",
    "        self.alpha = alpha\n",
    "         \n",
    "    def aggregate(self, weights):\n",
    "        weights_term = [np.exp(self.alpha * self.losses[i])* weights \\\n",
    "                        for i in range(self.num_clients)]\n",
    "        weights_term = list(weights_term / np.sum(weights_term))\n",
    "        super(TERM, self).aggregate(weights=weights_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e5d3634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(algorithm='qFedAvg', batch_size=64, data_dir='../data/MNIST/iid-4', dataset='MNIST', device='7', learning_rate=0.1, num_clients=4, num_epochs=10, num_local_epochs=1, num_workers=0, save_epoch=5, seed=0)\n",
      "output_dir:  ../results/MNIST/iid-4/FedAvg/batch_size_64/seed_0\n",
      "total train samples: 30000\n",
      "total test samples: 30000\n",
      "total samples: 60000\n",
      "samples:  [7500 7500 7500 7500]\n",
      "global epoch: 6\n",
      "losses: [0.103, 0.1055, 0.0898, 0.0811]\n",
      "global epoch: 7\n",
      "losses: [0.0849, 0.0854, 0.0809, 0.0587]\n",
      "global epoch: 8\n",
      "losses: [0.1117, 0.1063, 0.1052, 0.08]\n",
      "global epoch: 9\n",
      "losses: [0.0804, 0.0797, 0.08, 0.0463]\n",
      "mean:  97.50666666666666 std:  0.18135294011647532\n",
      "accs: [97.547, 97.76, 97.253, 97.467]\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "from torch import optim, nn\n",
    "from torch.utils.data import DataLoader\n",
    "import pickle\n",
    "import threading\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from utils.io import Tee, to_csv\n",
    "from utils.eval import accuracy, accuracies, losses\n",
    "from utils.aggregate import aggregate, aggregate_lr, zero_model, aggregate_momentum\n",
    "from algs.individual_train import individual_train\n",
    "from utils.concurrency import multithreads\n",
    "from models.models import resnet18, CNN, CNN_FEMNIST, RNN_Shakespeare, RNN_StackOverflow\n",
    "from utils.print import print_acc, round_list\n",
    "from utils.save import save_acc_loss\n",
    "from utils.stat import mean_std\n",
    "\n",
    "\n",
    "root = '..' \n",
    "\n",
    "parser = argparse.ArgumentParser(description='training')\n",
    "parser.add_argument('--device', type=str, default='7')\n",
    "parser.add_argument('--data_dir', type=str, default='iid-4')\n",
    "parser.add_argument('--dataset', type=str, default='MNIST')\n",
    "parser.add_argument('--algorithm', type=str, default='qFedAvg')\n",
    "parser.add_argument('--num_clients', type=int, default=4)\n",
    "parser.add_argument('--batch_size', type=int, default=64)\n",
    "parser.add_argument('--num_workers', type=int, default=0, help='for data loader')\n",
    "parser.add_argument('--num_epochs', type=int, default=10)\n",
    "parser.add_argument('--num_local_epochs', type=int, default=1)\n",
    "parser.add_argument('--learning_rate', type=float, default=0.1)\n",
    "parser.add_argument('--save_epoch', type=int, default=5)\n",
    "parser.add_argument('--seed', type=int, default=0)\n",
    "\n",
    "\n",
    "args = parser.parse_args('')\n",
    "output_dir = os.path.join(root, 'results', args.dataset, args.data_dir, 'FedAvg', \\\n",
    "                          f'batch_size_{args.batch_size}', f'seed_{args.seed}')\n",
    "args.data_dir = os.path.join(root, 'data', args.dataset, args.data_dir)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(args)\n",
    "print('output_dir: ', output_dir)\n",
    "\n",
    "with open(os.path.join(output_dir, 'args.json'), 'w') as fp:\n",
    "    json.dump(vars(args), fp)\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = args.device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')  # use the first GPU\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "in_file = os.path.join(args.data_dir, 'in.pickle')\n",
    "out_file = os.path.join(args.data_dir, 'out.pickle')\n",
    "\n",
    "with open(in_file, 'rb') as f_in:\n",
    "    in_data = pickle.load(f_in)\n",
    "with open(out_file, 'rb') as f_out:\n",
    "    out_data = pickle.load(f_out)  \n",
    "\n",
    "weights = np.array([len(in_data[i]) for i in range(args.num_clients)])\n",
    "weights_test = np.array([len(out_data[i]) for i in range(args.num_clients)])\n",
    "\n",
    "print('total train samples: {}'.format(np.sum(weights)))\n",
    "print('total test samples: {}'.format(np.sum(weights_test)))\n",
    "print('total samples: {}'.format(np.sum(weights)+np.sum(weights_test)))\n",
    "\n",
    "print('samples: ', weights)\n",
    "weights = list(weights / np.sum(weights))\n",
    "\n",
    "# data loaders\n",
    "train_loaders = [DataLoader(\n",
    "    dataset=in_data[i],\n",
    "    batch_size=args.batch_size,\n",
    "    num_workers=args.num_workers, drop_last=False, pin_memory=True, shuffle=True)\n",
    "    for i in range(args.num_clients)]\n",
    "\n",
    "test_loaders = [DataLoader(\n",
    "    dataset=out_data[i],\n",
    "    batch_size=args.batch_size,\n",
    "    num_workers=args.num_workers, drop_last=False, pin_memory=True, shuffle=True)\n",
    "    for i in range(args.num_clients)]\n",
    "\n",
    "if args.dataset == 'MNIST':\n",
    "    models = [CNN().to(device) for _ in range(args.num_clients)]\n",
    "elif args.dataset == 'CIFAR10':\n",
    "    models = [resnet18(num_classes=10).to(device)  for _ in range(args.num_clients)]\n",
    "elif args.dataset == 'CIFAR100':\n",
    "    models = [resnet18(num_classes=100).to(device)  for _ in range(args.num_clients)]\n",
    "elif args.dataset == 'CINIC10':\n",
    "    models = [resnet18(num_classes=10).to(device)  for _ in range(args.num_clients)]\n",
    "elif args.dataset == 'FEMNIST':\n",
    "    models = [CNN_FEMNIST().to(device) for _ in range(args.num_clients)]      \n",
    "elif args.dataset == 'Shakespeare':\n",
    "    models = [RNN_Shakespeare().to(device)  for _ in range(args.num_clients)]\n",
    "elif args.dataset == 'StackOverflow':\n",
    "    models = [RNN_StackOverflow().to(device)  for _ in range(args.num_clients)]     \n",
    "\n",
    "# loss functions, optimizer\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "#loss_func = nn.MSELoss()\n",
    "optimizers = [optim.SGD(model.parameters(), lr = args.learning_rate, \\\n",
    "                        momentum=0.0) for model in models]\n",
    "\n",
    "# checkpoint\n",
    "model_path = output_dir  + f'/model_last.pth'\n",
    "if os.path.exists(model_path):\n",
    "    start_epoch = torch.load(model_path)['epoch']\n",
    "    for model in models:\n",
    "        model.load_state_dict(torch.load(model_path)['state_dict'])\n",
    "else:\n",
    "    start_epoch = 0\n",
    "\n",
    "json_file = os.path.join(output_dir, 'log.json')\n",
    "with open(json_file, 'w') as f:\n",
    "    f.write('')\n",
    "\n",
    "if args.algorithm == 'FedAvg':\n",
    "    alg = FedAvg(models, optimizers, args.num_clients, args.num_local_epochs, loss_func)\n",
    "elif args.algorithm == 'AFL':\n",
    "    alg = AFL(models, optimizers, args.num_clients, args.num_local_epochs, loss_func, \\\n",
    "              lambda_=weights, step_size_lambda=0.1)\n",
    "elif args.algorithm == 'PropFair':\n",
    "    alg = PropFair(models, optimizers, args.num_clients, args.num_local_epochs, \\\n",
    "             loss_func, base=5.0, epsilon = 0.2)\n",
    "elif args.algorithm == 'qFFL':\n",
    "    alg = qFFL(models, optimizers, args.num_clients, args.num_local_epochs, \\\n",
    "             loss_func)\n",
    "elif args.algorithm == 'qFedAvg':\n",
    "    alg = qFedAvg(models, optimizers, args.num_clients, args.num_local_epochs, \\\n",
    "             loss_func, Lipschitz = 1 / args.learning_rate)\n",
    "else:\n",
    "    raise NotImplemented\n",
    "\n",
    "mean_accs = []\n",
    "for t in range(start_epoch + 1, args.num_epochs):\n",
    "    \n",
    "    alg.local_updates(train_loaders, test_loaders, device, output_dir)\n",
    "    alg.aggregate(weights=weights)\n",
    "    \n",
    "    accs = accuracies(alg.models, test_loaders, device)\n",
    "    losses_ = losses(alg.models, train_loaders, loss_func, device)\n",
    "    print(f'global epoch: {t}')\n",
    "    mean, std = mean_std(accs)\n",
    "    mean_accs.append(mean)\n",
    "    print(f'losses: {round_list(losses_)}')\n",
    "    save_acc_loss(json_file, t, accs, losses_)\n",
    "    if t % args.save_epoch == 0:\n",
    "        torch.save({'epoch': t, 'state_dict': models[0].state_dict()}, \\\n",
    "            output_dir  + f'/model_last.pth')\n",
    "    \n",
    "mean, std = mean_std(accs)\n",
    "print('mean: ', mean, 'std: ', std)\n",
    "print(f'accs: {[round(i, 3) for i in accs]}')\n",
    "\n",
    "acc_file = \"mean_acc.pkl\".format(args.dataset, args.seed)\n",
    "acc_file = os.path.join(output_dir, acc_file)\n",
    "\n",
    "with open(acc_file, 'wb') as f_out:\n",
    "    pickle.dump(mean_accs, f_out) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
